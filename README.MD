Flujo del Proyecto en Minikube ... Done
1. Preparaci√≥n del entorno  .. Done

Verificar que tienes Minikube y kubectl funcionando.  .. Done
Crear un Namespace exclusivo para el proyecto (mi-app). Esto mantendr√° todos los recursos aislados. .. Done

üëâ Raz√≥n: trabajar con Namespaces te da la mentalidad de entornos separados (como dev/prod). .. Done

2. Configurar la base de datos ..done

Crear un PersistentVolumeClaim (PVC) para almacenar datos. .. done

Crear un Deployment de la base de datos (MySQL). .. done

Exponerlo con un Service tipo ClusterIP para que sea accesible solo dentro del cluster. ..done

üëâ Raz√≥n: la base de datos debe tener almacenamiento persistente y no debe ser expuesta p√∫blicamente. ..done

3. Crear ConfigMaps y Secrets .. done

ConfigMap ‚Üí variables de configuraci√≥n no sensibles (nombre de la BD, host, etc.). ..done

Secret ‚Üí credenciales de la base de datos. ..done

üëâ Raz√≥n: separar configuraci√≥n y secretos del c√≥digo para buenas pr√°cticas de seguridad y portabilidad. ..done

4. Configurar el backend

Crear un Deployment para tu aplicaci√≥n backend (ej. Django, Flask, Node.js). ..done

Enlazar las variables de entorno al ConfigMap y Secret. ..done

Exponerlo internamente con un Service tipo ClusterIP.

üëâ Raz√≥n: el backend debe poder comunicarse con la DB usando los servicios internos de Kubernetes.

5. Exponer el backend al exterior

Tienes dos caminos:

Opci√≥n 1 (m√°s simple): usar un Service tipo NodePort y acceder v√≠a minikube service.

Opci√≥n 2 (m√°s realista): instalar un Ingress Controller en Minikube y crear un recurso Ingress para exponer la app en una URL (http://miapp.local).

üëâ Raz√≥n: NodePort es f√°cil para pruebas, Ingress se usa en entornos de producci√≥n.

6. Probar la aplicaci√≥n

Crear una base de datos en el contenedor DB si es necesario (ejecutando un init script o kubectl exec).

Confirmar conectividad:

Backend ‚Üí DB ‚úÖ

Cliente (navegador) ‚Üí Backend ‚úÖ

7. Opcionales para mejorar el proyecto

Agregar un frontend (React/Vue) como otro Deployment.

Usar un HorizontalPodAutoscaler (HPA) para escalar el backend seg√∫n la carga.

A√±adir livenessProbe y readinessProbe al backend.

Simular un fallo de pod para ver c√≥mo el Deployment mantiene disponibilidad.

üß≠ Resumen del flujo

Preparar Minikube y Namespace.

Desplegar base de datos (Deployment + PVC + Service).

ConfigMaps y Secrets.

Desplegar backend (Deployment + Service).

Exponer backend al exterior (NodePort o Ingress).

Probar funcionamiento end-to-end.

Extras opcionales.


## Documentaci√≥n

### 1. Namespace:
La infraestructura de la aplicaci√≥n esta en un entorno aislado (namespace) donde estar√°n algunos recursos comunicandose entre s√≠.
El namespace de este proyecto est√° en el directorio: /namespace que a nivel de todo el cluster se le llamar√° "my-app".

### 2. Persistent Volume (PV):
Para la base de datos primero se necesita de un lugar donde el pod_bd pueda gestionar los datos.
En este proyecto se usa un persistent volume que mantiene los datos aun si el pod cae, reinicia, etc. Conocido como ciclo de vida del Pod.
En este proyecto el persistent volume esta en el directorio volumes/volume_app.yaml
El PVC esta configirado para entornos local como manual que necesita de una configuraci√≥n manual de un PVC para servir como puente entre el PV y el Pod.
En entornos reales de producci√≥n no es recomendable usar la configuraci√≥n manual.
Algo que aclarar es el recurso PV no vive dentro de un namespace espec√≠fico, sino de forma global en el cluster.

### 3. Persistent Volume Claim (PVC):
Como se mencion√≥ en la parte de PV, se necesitar√° de un puente de conexi√≥n entre el Pod_db y el PV, es por eso que se configura un PVC. En este caso se configura en el directorio volumes/pvc.yaml, este objeto si vive dentro de el namespace "my-app" y con las mismas caracter√≠sticas del PV, as√≠ Kubernetes sabe con cu√°l hacer la conexi√≥n.

### 4. Deployment de la Base de Datos:
Un deployment permitir√° tener una replica de un Pod que correr√° un contenedor con la imagen de MySql. Ser√° necesario configurar algunos par√°metros mas, como el lugar donde se guaradar√°n los datos, es para esto que se crear√≥n el PV y PVC, para conexi√≥n solo es necesario especificar cu√°l es el PVC que usar√° nuestro Pod como volume. En entornos de Base de Datos tanbi√©n se necesitan de credenciales como el usuario y la contrase√±a, pero para buenas pr√°cticas de seguridad se usr√°n otros objetos. Cabe recalcar que un deployment si es un objeto Namespaced, por lo tanto este estar√° viviendo en nuestro Namespace "my-app".

### 5. ConfigMap y Secrets para el Pod DB:
Como se mencion√≥ anteriormente en el deployment de la base de datos, se necesitar√° configurar algunos param√©tros como credenciales.
Esto se configur√≥ en el deployment de db en "envFrom" que llama a dos recursos: ConfigMap y Secret. El recurso configMap se encuentra en el directorio configMap/config_map_deployment_db.yaml y contiene variables de entorno que son configuraci√≥n no sensible como el nombre de la db y el usuario.
En el caso de variables sensibles como contrase√±as de user y root de la bd se usa los secrets en el directorio secrets/secret_deployment_db.yaml donde los datos estan en base 64.

### 6. Service (ClusterIP) para deployment DB:
En una infraestructura segura, un servicio de base de datos nunca se expone de manera p√∫blica, solo algunos recursos como las aplicaciones pueden interactuar con la base de datos, en Kubernetes, dentro del cluster nuestra DB necesitar√° abrir un puerto de comunicaci√≥n interna solo dentro del cluster y en este caso en el namespace "my-app", los pods en el mismo namespace puede conectarse con el nombre del service, pero los que no se encuentran en el mismo namespace pero si en el cluster tendr√≠an que usar FQDN (no aplicable en este caso que solo trabajamos dentro de un mismo namespace). El service CLusterIP permite a un pod abrir un puerto interno para comunicaci√≥n, a trav√©s de una VIP (virtual ip) que como se mencion√≥ nadie fuera del cluster podr√° comunicarse con este. 

### 7. Deployment del Backend:
Para alojar el backend de la aplicaci√≥n, usaremos un deployment [/deployments/backend_deployment.yaml] con 2 replicas que levantar√° 2 Pods, cada uno con un container con la imagen de mi aplicaci√≥n backend de pruebas que esta escrito con Flask, es una API Rest CRUD b√°sica, que estar√° en Docker Hub de mi cuenta y ser√° p√∫blica. Tambi√©n se le agrega las variables de entorno de la base de datos para que pueda interactuar con esta. Tanto como los secrets y configMaps de la BD tambi√©n se usar√°n como variables de entorno, y tambi√©n el service CLusterIP que permite exponer el deploy de la BD dentro del cluster, con esto nuestro backend puede iniciar la comunicaci√≥n interna.

### 8. Service (Cluster IP) para el deploy Backend_
Seg√∫n la arquitectura, nuestra aplicaci√≥n es una API Rest, la aplicaci√≥n ya hace conexi√≥n con la base de datos por medio del service ClusterIP, pero ahora se necesita que el backend tambipen se exponga dentro del cluster para ser consumido por el deployment del Frontend que se mostrar√° mas adelante. Es por eso que tambien con un Service ClusterIP podemos hacer esto. El service est√° en "services/clusterip_deployment_backend.yaml".


